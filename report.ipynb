{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BBM469 : Data Intensive Applications Lab. Assignment 3\n",
    "21827497 - Alperen Berk IŞILDAR\n",
    "21827373 - Uğurcan ERDOĞAN\n",
    "#### Pre-Note\n",
    "###### Graphs and Tables will be updated on new run.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Table of Content\n",
    "\n",
    "[Purpose](#purpose)\n",
    "[Data Understanding](#data_understanding)\n",
    "[Data Preparation](#data_preparation)\n",
    "[Modeling for Clustering](#clusteringmodel)\n",
    "[Clustering results](#clustering)\n",
    "[Modeling for Classification](#classificationmodel)\n",
    "[Evaluation](#evaluation)\n",
    "[References](#references)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Purpose <a class=\"anchor\" id=\"purpose\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### PROBLEM DEFINITION :\n",
    "In this assignment, we were asked to understand the classification and clustering algorithms using Apache Spark library for Python and perform a basic experiment with appropriate datasets. We have also dealt with data manipulation and data normalization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setting the objects spark and sc as stated in DataCamp\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuZmF4SvpGMc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Understanding<a class=\"anchor\" id=\"data_understanding\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path is:  data.csv\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, DESKTOP-JIRLH52.mshome.net, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:465)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1934)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:465)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1934)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_18368/323409949.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mfileRDD\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtextFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfileName\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"File path is: \"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfileName\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfileRDD\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Documents\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001B[0m in \u001B[0;36mtake\u001B[1;34m(self, num)\u001B[0m\n\u001B[0;32m   1444\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1445\u001B[0m             \u001B[0mp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpartsScanned\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpartsScanned\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mnumPartsToTry\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotalParts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1446\u001B[1;33m             \u001B[0mres\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtakeUpToNumLeft\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1447\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1448\u001B[0m             \u001B[0mitems\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\context.py\u001B[0m in \u001B[0;36mrunJob\u001B[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001B[0m\n\u001B[0;32m   1118\u001B[0m         \u001B[1;31m# SparkContext#runJob.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1119\u001B[0m         \u001B[0mmappedRDD\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrdd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpartitionFunc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1120\u001B[1;33m         \u001B[0msock_info\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpartitions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1121\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1122\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1302\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1304\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1306\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    126\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    127\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 128\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    129\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    130\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 326\u001B[1;33m                 raise Py4JJavaError(\n\u001B[0m\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, DESKTOP-JIRLH52.mshome.net, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:465)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1934)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:465)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1934)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n"
     ]
    }
   ],
   "source": [
    "# Setting up rdd file and check if we created it correctly\n",
    "fileName = \"data.csv\"\n",
    "fileRDD = sc.textFile(fileName)\n",
    "print(\"File path is: \", fileName)\n",
    "print(fileRDD.take(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating data frame from given csv file and check it\n",
    "OD = spark.read.csv(fileName, header=True, inferSchema=True)\n",
    "OD.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word Count (Task 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# in order not to spoil data frame we have, created a new data frame and split by \",\" to create an array includes eah element in the rdd\n",
    "# and take a look at to the top.\n",
    "wordCountRDD = fileRDD.flatMap(lambda f: f.split(\",\"))\n",
    "wordCountRDD.take(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rdd is done as key value pairs and take a look at\n",
    "wordCountRDD = wordCountRDD.map(lambda f: (f,1))\n",
    "wordCountRDD.take(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sum up the values that have the same values and sort them by using the values and take a look at\n",
    "wordCountRDD = wordCountRDD.reduceByKey(lambda f,h: f+h)\n",
    "wordCountRDD = wordCountRDD.sortBy(lambda f: f[1], ascending=False)\n",
    "wordCountRDD.take(7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Checking if the rdd file is created correctly and total line count\n",
    "print(\"fileRDD type is: \", type(fileRDD))\n",
    "print(\"Total number of lines is: \", fileRDD.count())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#total number of rows\n",
    "print(\"total number of rows:\", OD.count())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Checking the column values' types\n",
    "OD.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Average values of each column is: \")\n",
    "OD.agg({\n",
    "    'Area': 'avg',\n",
    "    'Perimeter': 'avg',\n",
    "    'MajorAxisLength': 'avg',\n",
    "    'MinorAxisLength': 'avg',\n",
    "    'AspectRation': 'avg',\n",
    "    'Eccentricity': 'avg',\n",
    "    'ConvexArea': 'avg',\n",
    "    'EquivDiameter': 'avg',\n",
    "    'Extent': 'avg',\n",
    "    'Solidity': 'avg',\n",
    "    'roundness': 'avg',\n",
    "    'Compactness':'avg',\n",
    "    'ShapeFactor1':'avg',\n",
    "    'ShapeFactor2':'avg',\n",
    "    'ShapeFactor3':'avg',\n",
    "    'ShapeFactor4':'avg'\n",
    "}).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Min values of each column is: \")\n",
    "OD.agg({\n",
    "    'Area': 'min',\n",
    "    'Perimeter': 'min',\n",
    "    'MajorAxisLength': 'min',\n",
    "    'MinorAxisLength': 'min',\n",
    "    'AspectRation': 'min',\n",
    "    'Eccentricity': 'min',\n",
    "    'ConvexArea': 'min',\n",
    "    'EquivDiameter': 'min',\n",
    "    'Extent': 'min',\n",
    "    'Solidity': 'min',\n",
    "    'roundness': 'min',\n",
    "    'Compactness':'min',\n",
    "    'ShapeFactor1':'min',\n",
    "    'ShapeFactor2':'min',\n",
    "    'ShapeFactor3':'min',\n",
    "    'ShapeFactor4':'min'\n",
    "}).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Max values of each column is: \")\n",
    "OD.agg({\n",
    "    'Area': 'max',\n",
    "    'Perimeter': 'max',\n",
    "    'MajorAxisLength': 'max',\n",
    "    'MinorAxisLength': 'max',\n",
    "    'AspectRation': 'max',\n",
    "    'Eccentricity': 'max',\n",
    "    'ConvexArea': 'max',\n",
    "    'EquivDiameter': 'max',\n",
    "    'Extent': 'max',\n",
    "    'Solidity': 'max',\n",
    "    'roundness': 'max',\n",
    "    'Compactness':'max',\n",
    "    'ShapeFactor1':'max',\n",
    "    'ShapeFactor2':'max',\n",
    "    'ShapeFactor3':'max',\n",
    "    'ShapeFactor4':'max'\n",
    "}).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Percentage of the classes\n",
    "print(\"DERMASON samples make up\", round(wordCountRDD.take(7)[0][1] / OD.count()* 100, 2), \"% of the dataset.\")\n",
    "print(\"SIRA samples make up\", round(wordCountRDD.take(7)[1][1] / OD.count()* 100, 2), \"% of the dataset.\")\n",
    "print(\"SEKER samples make up\", round(wordCountRDD.take(7)[2][1] / OD.count()* 100, 2), \"% of the dataset.\")\n",
    "print(\"HOROZ samples make up\", round(wordCountRDD.take(7)[3][1] / OD.count()* 100, 2), \"% of the dataset.\")\n",
    "print(\"CALI samples make up\", round(wordCountRDD.take(7)[4][1] / OD.count()* 100, 2), \"% of the dataset.\")\n",
    "print(\"BARBUNYA samples make up\", round(wordCountRDD.take(7)[5][1] / OD.count()* 100, 2), \"% of the dataset.\")\n",
    "print(\"BOMBAY samples make up\", round(wordCountRDD.take(7)[6][1] / OD.count()* 100, 2), \"% of the dataset.\")\n",
    "# Plotting results\n",
    "sns.countplot(x=\"Class\", data=OD.toPandas())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation<a class=\"anchor\" id=\"data_preparation\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# \"Class\" column has categorical string values and this makes our model cannot be trained\n",
    "# We are using StringIndexer to handle this situation.\n",
    "# and also drop the \"Class\" column..\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Class\", outputCol=\"ClassIndex\")\n",
    "OD = indexer.fit(OD).transform(OD)\n",
    "OD = OD.drop(\"Class\")\n",
    "OD.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " StringIndexer indexes categorical variables by their frequency so in this case:\n",
    " 0: DERMASON, 1: SIRA, 2: SEKER, 3: HOROZ, 4: CALI, 5: BARBUNYA, 6: BOMBAY"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# As we can see above, some values are null so we should drop that rows all.\n",
    "# We googled that why are nullables true still but we found that PySpark dataframes has a bug for nullables bool values.\n",
    "# We can check the number of total rows and compare it to the default number of rows and we can see that data has no null values\n",
    "OD = OD.na.drop(how='any')\n",
    "OD.printSchema()\n",
    "\n",
    "print(\"Total sample number: {}\".format(OD.count()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# All of the columns have double values except by \"Area\" and \"ConvexArea\" so we should convert them into double too.\n",
    "\n",
    "OD = OD.withColumn(\"Area\", OD[\"Area\"].cast(DoubleType()))\n",
    "OD = OD.withColumn(\"ConvexArea\", OD[\"ConvexArea\"].cast(DoubleType()))\n",
    "OD.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "ND = OD.select(\"*\")\n",
    "\n",
    "# Creating normalized dataset(ND)\n",
    "columns_to_normalize = [\"Area\", \"EquivDiameter\", \"MinorAxisLength\", \"ConvexArea\", \"AspectRation\", \"Perimeter\", \"MajorAxisLength\"]\n",
    "assembler = [VectorAssembler(inputCols=[col], outputCol=col+\"_Vect\") for col in columns_to_normalize]\n",
    "scaler = [MinMaxScaler(inputCol=col+\"_Vect\", outputCol=col+\"_Scaled\") for col in columns_to_normalize]\n",
    "pipeline = Pipeline(stages=assembler + scaler)\n",
    "\n",
    "NDModel = pipeline.fit(ND)\n",
    "ND = NDModel.transform(ND)\n",
    "\n",
    "for col in columns_to_normalize:\n",
    "    ND = ND.drop(col+\"_Vect\")\n",
    "    ND = ND.drop(col)\n",
    "\n",
    "\n",
    "firstElem = F.udf(lambda v: float(v[0]), FloatType())\n",
    "NDX = ND.select([firstElem(col+\"_Scaled\").alias(col+\"_Scaled\") for col in columns_to_normalize])\n",
    "for col in columns_to_normalize:\n",
    "#     print(col)\n",
    "#     if col not in ND.schema.names:\n",
    "#         firstElem(col+\"_Scaled\").alias(col+\"_Scaled\")\n",
    "    NDX = NDX.withColumn(col+\"_Scaled\", F.col(col+\"_Scaled\").cast(DoubleType()))\n",
    "\n",
    "for col in columns_to_normalize:\n",
    "    ND = ND.drop(col+\"_Scaled\")\n",
    "\n",
    "def append_ODs(df2,df3):\n",
    "    DF1 = df2.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "    DF2 = df3.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "    result_df = DF1.join(DF2, (\"row_id\")).drop(\"row_id\")\n",
    "    return result_df\n",
    "\n",
    "ND = append_ODs(ND,NDX)\n",
    "ND.show()\n",
    "ND.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5yg37-IpGM1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modeling for Clustering <a class=\"anchor\" id=\"clusteringmodel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "feat_cols = []\n",
    "\n",
    "for i in OD.columns:\n",
    "    OD = OD.withColumn(i, F.col(i).cast(\"float\"))\n",
    "\n",
    "for i in OD.schema.names:\n",
    "    if i != \"ClassIndex\":\n",
    "        feat_cols.append(i)\n",
    "\n",
    "x_OD = OD.select(feat_cols)\n",
    "vec_assembler_OD = VectorAssembler(inputCols = x_OD.schema.names, outputCol='features')\n",
    "final_data_OD = vec_assembler_OD.transform(x_OD)\n",
    "\n",
    "kmeans_OD = KMeans(featuresCol=\"features\",k=7)\n",
    "kmeans_model_OD = kmeans_OD.fit(final_data_OD)\n",
    "kmeans_final_OD = kmeans_model_OD.transform(final_data_OD)\n",
    "\n",
    "pca_OD = PCA(k=2, inputCol=\"features\", outputCol=\"pca\")\n",
    "pca_model_OD = pca_OD.fit(final_data_OD)\n",
    "pca_final_OD = pca_model_OD.transform(final_data_OD)\n",
    "x_pca_OD = np.array(pca_final_OD.rdd.map(lambda row: row.pca).collect())\n",
    "x_kmeans_OD = np.array(kmeans_final_OD.rdd.map(lambda row: row.prediction).collect()).reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_cols = []\n",
    "\n",
    "for i in ND.columns:\n",
    "    ND = ND.withColumn(i, F.col(i).cast(\"float\"))\n",
    "\n",
    "for i in ND.schema.names:\n",
    "    if i != \"ClassIndex\":\n",
    "        feat_cols.append(i)\n",
    "x_ND = ND.select(feat_cols)\n",
    "vec_assembler_ND = VectorAssembler(inputCols = x_ND.schema.names, outputCol='features')\n",
    "final_data_ND = vec_assembler_ND.transform(x_ND)\n",
    "\n",
    "kmeans_ND = KMeans(featuresCol=\"features\",k=7)\n",
    "kmeans_model_ND = kmeans_ND.fit(final_data_ND)\n",
    "kmeans_final_ND = kmeans_model_ND.transform(final_data_ND)\n",
    "\n",
    "pca_ND = PCA(k=2, inputCol=\"features\", outputCol=\"pca\")\n",
    "pca_model_ND = pca_ND.fit(final_data_ND)\n",
    "pca_final_ND = pca_model_ND.transform(final_data_ND)\n",
    "\n",
    "x_pca_ND = np.array(pca_final_ND.rdd.map(lambda row: row.pca).collect())\n",
    "x_kmeans_ND = np.array(kmeans_final_ND.rdd.map(lambda row: row.prediction).collect()).reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set Clustered Original Dataset(COD)\n",
    "COD = OD.select(\"*\")\n",
    "COD = COD.drop(\"ClassIndex\")\n",
    "COD = append_ODs(COD, kmeans_final_OD.select(\"prediction\"))\n",
    "COD = COD.withColumnRenamed(\"prediction\", \"ClassIndex\")\n",
    "COD.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set Clustered Normalized Dataset(CND)\n",
    "CND = ND.select(\"*\")\n",
    "CND = CND.drop(\"ClassIndex\")\n",
    "CND = append_ODs(CND, kmeans_final_ND.select(\"prediction\"))\n",
    "CND = CND.withColumnRenamed(\"prediction\", \"ClassIndex\")\n",
    "CND.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_cols = []\n",
    "\n",
    "for i in COD.schema.names:\n",
    "    if i != \"ClassIndex\":\n",
    "        feat_cols.append(i)\n",
    "\n",
    "x_COD = COD.select(feat_cols)\n",
    "vec_assembler_COD = VectorAssembler(inputCols = x_COD.schema.names, outputCol='features')\n",
    "final_data_COD = vec_assembler_COD.transform(x_COD)\n",
    "\n",
    "kmeans_COD = KMeans(featuresCol=\"features\",k=7)\n",
    "kmeans_model_COD = kmeans_COD.fit(final_data_COD)\n",
    "kmeans_final_COD = kmeans_model_COD.transform(final_data_COD)\n",
    "\n",
    "pca_COD = PCA(k=2, inputCol=\"features\", outputCol=\"pca\")\n",
    "pca_model_COD = pca_COD.fit(final_data_COD)\n",
    "pca_final_COD = pca_model_COD.transform(final_data_COD)\n",
    "x_pca_COD = np.array(pca_final_COD.rdd.map(lambda row: row.pca).collect())\n",
    "x_kmeans_COD = np.array(kmeans_final_COD.rdd.map(lambda row: row.prediction).collect()).reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_cols = []\n",
    "\n",
    "for i in CND.columns:\n",
    "    CND = CND.withColumn(i, F.col(i).cast(\"float\"))\n",
    "\n",
    "for i in CND.schema.names:\n",
    "    if i != \"ClassIndex\":\n",
    "        feat_cols.append(i)\n",
    "x_CND = CND.select(feat_cols)\n",
    "vec_assembler_CND = VectorAssembler(inputCols = x_CND.schema.names, outputCol='features')\n",
    "final_data_CND = vec_assembler_CND.transform(x_CND)\n",
    "\n",
    "kmeans_CND = KMeans(featuresCol=\"features\",k=7)\n",
    "kmeans_model_CND = kmeans_CND.fit(final_data_CND)\n",
    "kmeans_final_CND = kmeans_model_CND.transform(final_data_CND)\n",
    "\n",
    "pca_CND = PCA(k=2, inputCol=\"features\", outputCol=\"pca\")\n",
    "pca_model_CND = pca_CND.fit(final_data_CND)\n",
    "pca_final_CND = pca_model_CND.transform(final_data_CND)\n",
    "\n",
    "x_pca_CND = np.array(pca_final_CND.rdd.map(lambda row: row.pca).collect())\n",
    "x_kmeans_CND = np.array(kmeans_final_CND.rdd.map(lambda row: row.prediction).collect()).reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering results<a class=\"anchor\" id=\"clustering\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_data_OD = np.hstack((x_pca_OD, x_kmeans_OD))\n",
    "pca_df_OD = pd.DataFrame(data=pca_data_OD, columns=(\"1st_principal\", \"2nd_principal\", \"cluster_assignment\"))\n",
    "sns.FacetGrid(pca_df_OD, hue=\"cluster_assignment\", size=8).map(plt.scatter, '1st_principal', '2nd_principal' ).add_legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_data_ND = np.hstack((x_pca_ND, x_kmeans_ND))\n",
    "pca_df_ND = pd.DataFrame(data=pca_data_ND, columns=(\"1st_principal\", \"2nd_principal\", \"cluster_assignment\"))\n",
    "sns.FacetGrid(pca_df_ND, hue=\"cluster_assignment\", size=8).map(plt.scatter, '1st_principal', '2nd_principal' ).add_legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_data_COD = np.hstack((x_pca_COD, x_kmeans_COD))\n",
    "pca_df_COD = pd.DataFrame(data=pca_data_COD, columns=(\"1st_principal\", \"2nd_principal\", \"ClassIndex\"))\n",
    "sns.FacetGrid(pca_df_COD, hue=\"ClassIndex\", size=8).map(plt.scatter, '1st_principal', '2nd_principal' ).add_legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_data_CND = np.hstack((x_pca_CND, x_kmeans_CND))\n",
    "pca_df_CND = pd.DataFrame(data=pca_data_CND, columns=(\"1st_principal\", \"2nd_principal\", \"ClassIndex\"))\n",
    "sns.FacetGrid(pca_df_CND, hue=\"ClassIndex\", size=8).map(plt.scatter, '1st_principal', '2nd_principal' ).add_legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Modeling for Classification <a class=\"anchor\" id=\"classificationmodel\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def classification_helper(data):\n",
    "    feat_cols_df = []\n",
    "    for i in data.schema.names:\n",
    "        if i != \"ClassIndex\":\n",
    "            feat_cols_df.append(i)\n",
    "\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feat_cols_df,outputCol=\"features\")\n",
    "    output = assembler.transform(data)\n",
    "    final_data_rfc = output.select('features','ClassIndex')\n",
    "\n",
    "    (training_data, test_data) = final_data_rfc.randomSplit([0.8, 0.2])\n",
    "\n",
    "    maxDepthList= list(range(5,6,5))\n",
    "\n",
    "    accuracy_list = []\n",
    "    rfc_predictions_list = []\n",
    "    for i in maxDepthList:\n",
    "\n",
    "        rfc = RandomForestClassifier(labelCol='ClassIndex',featuresCol='features',maxDepth=i)\n",
    "\n",
    "        rfc_model = rfc.fit(final_data_rfc)\n",
    "        rfc_predictions = rfc_model.transform(test_data)\n",
    "        rfc_predictions_list.append(rfc_predictions)\n",
    "        # Select (prediction, true label) and compute test error\n",
    "        rfc_evaluator = MulticlassClassificationEvaluator(labelCol=\"ClassIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        accuracy = rfc_evaluator.evaluate(rfc_predictions)\n",
    "        accuracy_list.append(accuracy*100)\n",
    "        print(\"Accuracy for our dataset is : {:.2f}%\".format(accuracy*100))\n",
    "\n",
    "    print(\"The best accuracy score is: {:.2f}% --- Optimal Max Depth hyperparameter: {} \".format(max(accuracy_list),(accuracy_list.index(max(accuracy_list))+1)*5))\n",
    "\n",
    "    plt.plot(maxDepthList, accuracy_list)\n",
    "    plt.title(\"Max Depth Scores\")\n",
    "    plt.xlabel(\"Max Depth value\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend([\"OD\",\"ND\",\"COD\",\"CND\"])\n",
    "\n",
    "    max_score_prediction = rfc_predictions_list.pop(accuracy_list.index(max(accuracy_list)))\n",
    "    #important: need to cast to float type, and order by prediction, else it won't work\n",
    "    preds_and_labels = max_score_prediction.select(['prediction','ClassIndex']).withColumn('ClassIndex', F.col('ClassIndex').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "    #select only prediction and label columns\n",
    "    preds_and_labels = preds_and_labels.select(['prediction','ClassIndex'])\n",
    "\n",
    "    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "    print(\"===Multiclass Confusion Matrix===\")\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "    return accuracy_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Classification results <a class=\"anchor\" id=\"classificationmodel\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classification_helper(OD)\n",
    "classification_helper(ND)\n",
    "classification_helper(COD)\n",
    "classification_helper(CND)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "bbm469_hw3_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}